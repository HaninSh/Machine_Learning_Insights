{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Selection\n"
      ],
      "metadata": {
        "id": "3p7cDamH104P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Needed Libraries"
      ],
      "metadata": {
        "id": "bSzKzg-l3Czi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "############ GENERAL LIBS ####################\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "############### FILTERING METHOD / CHI-SQUARE ##################\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "############### WRAPPER METHOD / FORWARD SELECTION & BACKWARD ELIMINATION #################\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "################ WRAPPER METHOD / RECURSIVE FEATURE ELIMINATION ###############\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LogisticRegression\n"
      ],
      "metadata": {
        "id": "o6xEupYd3GXO"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filter Method"
      ],
      "metadata": {
        "id": "KHEy5BY52e11"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Chi-Square Test"
      ],
      "metadata": {
        "id": "wo955XIW2x71"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1:**\n",
        "\n",
        "**Load Data:**\n",
        "\n",
        "We use the Iris dataset and split the data into features (X) and target (y).\n",
        "\n",
        "**Step 2:**\n",
        "\n",
        "**Normalize Data:**\n",
        "\n",
        "Chi-square requires non-negative data, so we take the absolute values of the features.\n",
        "\n",
        "**Step 3:**\n",
        "\n",
        "**Apply SelectKBest:**\n",
        "\n",
        "This filter method uses the Chi-Square test to select the top 2 features that are most related to the target variable.\n",
        "\n",
        "**Step 4:**\n",
        "\n",
        "*   **Fit and Transform:** We fit the selector on the data and transform it to select the best features.\n",
        "*   We print the selected top 2 features based on the Chi-square score.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "N8jP3fbF6mOD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOCM_aHA1XGp",
        "outputId": "7a76738c-054b-4e6e-b42b-3e15984c5ba6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original features:\n",
            " Index(['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)',\n",
            "       'petal width (cm)'],\n",
            "      dtype='object')\n",
            "\n",
            "Selected top 2 features:\n",
            " Index(['petal length (cm)', 'petal width (cm)'], dtype='object')\n"
          ]
        }
      ],
      "source": [
        "# Load a sample dataset (Iris)\n",
        "iris = load_iris()\n",
        "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "y = iris.target\n",
        "\n",
        "# Apply SelectKBest with Chi-square test to select the top 2 features\n",
        "X_normalized = np.abs(X)\n",
        "\n",
        "# Select the best 2 features based on Chi-square test\n",
        "selector = SelectKBest(score_func=chi2, k=2)\n",
        "X_selected = selector.fit_transform(X_normalized, y)\n",
        "\n",
        "# Display the selected features\n",
        "print(\"Original features:\\n\", X.columns)\n",
        "print(\"\\nSelected top 2 features:\\n\", X.columns[selector.get_support()])\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Correlation"
      ],
      "metadata": {
        "id": "w5mAU5x723FQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1:**\n",
        "\n",
        "**Load Data:**\n",
        "\n",
        "We load the Iris dataset and split it into **features (X)** and **target (y)**.\n",
        "\n",
        "**Step 2:**\n",
        "\n",
        "**Calculate Correlation:**\n",
        "\n",
        "*   We compute the correlation between each feature and the target using the **np.corrcoef function**\n",
        "*   We take the absolute values of the correlations because we are interested in the strength, not the direction.\n",
        "\n",
        "**Step 3:**\n",
        "\n",
        "**Select Features:**\n",
        "\n",
        "\n",
        "*    We define a correlation threshold (0.5 in this case) and select the features with correlation values greater than the threshold.\n",
        "*   We print the selected features\n",
        "\n",
        "\n",
        "**Step 4:**\n",
        "\n",
        "Split the data into training and test sets using only the selected features."
      ],
      "metadata": {
        "id": "RPNsfaC544Lc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a sample dataset (Iris)\n",
        "iris = load_iris()\n",
        "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "y = pd.Series(iris.target)\n",
        "\n",
        "# Compute correlation of each feature with the target\n",
        "correlations = X.apply(lambda feature: np.abs(np.corrcoef(feature, y)[0, 1]))\n",
        "\n",
        "# Select features with high correlation (Set a threshold for this example I chose 0.5)\n",
        "threshold = 0.5\n",
        "selected_features = correlations[correlations > threshold].index\n",
        "X_selected = X[selected_features]\n",
        "\n",
        "# Display the selected features based on correlation\n",
        "print(\"Original features:\\n\", X.columns)\n",
        "print(\"\\nSelected features with correlation > 0.5:\\n\", selected_features)\n",
        "\n",
        "# Split data into train and test sets using the selected features\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLbNLnh_2_W9",
        "outputId": "1cbf857e-8d84-4434-e0b3-fa14c4993de8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original features:\n",
            " Index(['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)',\n",
            "       'petal width (cm)'],\n",
            "      dtype='object')\n",
            "\n",
            "Selected features with correlation > 0.5:\n",
            " Index(['sepal length (cm)', 'petal length (cm)', 'petal width (cm)'], dtype='object')\n",
            "\n",
            "Shape of X_train with selected features: (105, 3)\n",
            "Shape of X_test with selected features: (45, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wrapper Method"
      ],
      "metadata": {
        "id": "2QIMOM2z7tfg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Forward Selection"
      ],
      "metadata": {
        "id": "HFXQ99He7v41"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, weâ€™ll use Scikit-learn's LinearRegression model and evaluate performance based on R-squared score to select the best features.\n",
        "\n",
        "**Step 1:**\n",
        "\n",
        "**Load Data:**\n",
        "\n",
        "We load the Iris dataset and split it into features (X) and target (y).\n",
        "\n",
        "**Step 2:**\n",
        "\n",
        "**Train-Test Split:**\n",
        "\n",
        "We split the data into training and testing sets to evaluate the model on unseen data.\n",
        "\n",
        "**Step 3:**\n",
        "\n",
        "**Initialize Forward Selection:**\n",
        "\n",
        "We start with an empty list of selected_features and a list of remaining_features.\n",
        "\n",
        "**Step 4:**\n",
        "\n",
        "**Evaluate Model Performance:**\n",
        "\n",
        "For each iteration, we:\n",
        "\n",
        "\n",
        "*   Add each remaining feature to the selected set.\n",
        "*   Train a LinearRegression model with the current set of selected features.\n",
        "*   Evaluate the model using the R-squared score on the test set.\n",
        "\n",
        "**Step 5:**\n",
        "\n",
        "**Select the Best Feature:**\n",
        "\n",
        "\n",
        "*   We select the feature that leads to the highest R-squared score\n",
        "*   add it to the selected_features\n",
        "*   remove it from remaining_features.\n",
        "\n",
        "\n",
        "**Output:** After all iterations, we print the final set of selected features."
      ],
      "metadata": {
        "id": "olmp5kc48QIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a sample dataset (Iris)\n",
        "iris = load_iris()\n",
        "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "y = iris.target\n"
      ],
      "metadata": {
        "id": "3Tvq-DfA8d8o"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
      ],
      "metadata": {
        "id": "8rN_-9C78iYr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "model = LinearRegression()\n"
      ],
      "metadata": {
        "id": "HNBEobzi8k8f"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward selection: start with no features\n",
        "selected_features = []\n",
        "remaining_features = list(X.columns)\n",
        "best_r2 = 0  # Initialize best R-squared score\n",
        "\n",
        "# Perform forward selection\n",
        "for i in range(len(remaining_features)):\n",
        "    r2_scores = []\n",
        "    for feature in remaining_features:\n",
        "        # Try adding each remaining feature to the selected set\n",
        "        features_to_try = selected_features + [feature]\n",
        "        model.fit(X_train[features_to_try], y_train)\n",
        "        y_pred = model.predict(X_test[features_to_try])\n",
        "        r2_scores.append(r2_score(y_test, y_pred))\n",
        "\n",
        "    # Select the feature that gives the best R-squared score\n",
        "    best_feature_idx = np.argmax(r2_scores)\n",
        "    best_feature = remaining_features[best_feature_idx]\n",
        "    best_r2 = r2_scores[best_feature_idx]\n",
        "\n",
        "    # Add the best feature to the selected features\n",
        "    selected_features.append(best_feature)\n",
        "    remaining_features.remove(best_feature)\n",
        "\n",
        "    # Print the feature added and the updated R-squared score\n",
        "    print(f\"Added feature: {best_feature}, R-squared score: {best_r2}\")\n",
        "\n",
        "# Final selected features\n",
        "print(\"\\nFinal selected features:\", selected_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7umZK-W18nH1",
        "outputId": "b023acc5-13a3-4850-8287-54dbae55918a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added feature: petal width (cm), R-squared score: 0.9456393705719633\n",
            "Added feature: sepal width (cm), R-squared score: 0.9461771742063361\n",
            "Added feature: petal length (cm), R-squared score: 0.9423839660334823\n",
            "Added feature: sepal length (cm), R-squared score: 0.9442318571467434\n",
            "\n",
            "Final selected features: ['petal width (cm)', 'sepal width (cm)', 'petal length (cm)', 'sepal length (cm)']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Backward Elimination"
      ],
      "metadata": {
        "id": "WqGD0O6A7ydt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1**:\n",
        "\n",
        "**Load Data**:\n",
        "\n",
        "We load the Iris dataset and split it into features (X) and target (y).\n",
        "\n",
        "**Step 2:**\n",
        "\n",
        "**Train-Test Split:**\n",
        "\n",
        "We split the data into training and testing sets to evaluate model performance on unseen data.\n",
        "\n",
        "**Step 3:**\n",
        "\n",
        "**Initialize Backward Elimination:**\n",
        "\n",
        "We start with all features (selected_features initialized to all feature names).\n",
        "\n",
        "**Step 4:**\n",
        "\n",
        "**Train Model with All Features:**\n",
        "\n",
        "Initially, we train a LinearRegression model using all features and record the R-squared score.\n",
        "\n",
        "**Step 5:**\n",
        "\n",
        "**Iterative Feature Removal:**\n",
        "\n",
        "*   We iteratively remove one feature at a time and evaluate the model's performance.\n",
        "*   After removing a feature, you check if the modelâ€™s performance (R-squared score) changes.\n",
        "*   If removing a feature improves the model's performance or only causes a small reduction in R-squared, this feature is likely not contributing much to the model.\n",
        "*   Once you find the feature whose removal has the least negative impact (or even improves the R-squared score), you remove it permanently from the model.\n",
        "*   Continue the process by removing one feature at a time in each iteration, always checking the modelâ€™s performance after each removal.\n",
        "\n",
        "**Step 6**\n",
        "\n",
        "**Stop Criterion:**\n",
        "\n",
        "The process stops when removing further features reduces model performance.\n"
      ],
      "metadata": {
        "id": "d1J4QHdnKccB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a sample dataset (Iris)\n",
        "iris = load_iris()\n",
        "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "y = iris.target\n"
      ],
      "metadata": {
        "id": "3IYdKTI1KH3q"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
      ],
      "metadata": {
        "id": "dOcX5FgEKJDr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "model = LinearRegression()\n"
      ],
      "metadata": {
        "id": "wXIVl0XPKMQN"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Backward elimination: start with all features\n",
        "selected_features = list(X.columns)\n",
        "best_r2 = 0  # Initialize the best R-squared score\n",
        "\n",
        "# Train the model with all features initially\n",
        "model.fit(X_train[selected_features], y_train)\n",
        "y_pred = model.predict(X_test[selected_features])\n",
        "best_r2 = r2_score(y_test, y_pred)\n",
        "print(f\"Initial R-squared score with all features: {best_r2}\")\n",
        "\n",
        "# Perform backward elimination\n",
        "while len(selected_features) > 1:\n",
        "    r2_scores = []\n",
        "\n",
        "    # Test removing each feature and train the model\n",
        "    for feature in selected_features:\n",
        "        features_to_try = selected_features.copy()\n",
        "        features_to_try.remove(feature)\n",
        "        model.fit(X_train[features_to_try], y_train)\n",
        "        y_pred = model.predict(X_test[features_to_try])\n",
        "        r2_scores.append(r2_score(y_test, y_pred))\n",
        "\n",
        "    # Find the worst feature (the one whose removal improves or minimally impacts performance)\n",
        "    worst_feature_idx = np.argmax(r2_scores)\n",
        "    worst_feature = selected_features[worst_feature_idx]\n",
        "\n",
        "    # Remove the feature if it improves or doesn't degrade performance much\n",
        "    if r2_scores[worst_feature_idx] >= best_r2:\n",
        "        best_r2 = r2_scores[worst_feature_idx]\n",
        "        print(f\"Removed feature: {worst_feature}, New R-squared score: {best_r2}\")\n",
        "        selected_features.remove(worst_feature)\n",
        "    else:\n",
        "        break  # Stop if removing features worsens the performance\n",
        "\n",
        "# Final selected features\n",
        "print(\"\\nFinal selected features:\", selected_features)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvbcbubtKQFu",
        "outputId": "09fed974-95a8-4f8b-8aa7-f5e36375ce15"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial R-squared score with all features: 0.9442318571467434\n",
            "Removed feature: sepal width (cm), New R-squared score: 0.9446990399987059\n",
            "\n",
            "Final selected features: ['sepal length (cm)', 'petal length (cm)', 'petal width (cm)']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Recursive Feature Elimination (RFE)"
      ],
      "metadata": {
        "id": "4gLjhS6v76NZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1**:\n",
        "\n",
        "**Load Data**:\n",
        "\n",
        "We load the Iris dataset and split it into features (X) and target (y).\n",
        "\n",
        "**Step 2:**\n",
        "\n",
        "**Train-Test Split:**\n",
        "\n",
        "We split the data into training and testing sets to evaluate model performance on unseen data.\n",
        "\n",
        "**Step 3:**\n",
        "\n",
        "**Initialize the Model:**\n",
        "\n",
        "We use a LogisticRegression model, but any model with feature importance measures can be used.\n",
        "\n",
        "**Step 4:**\n",
        "\n",
        "**Apply RFE:**\n",
        "\n",
        "* We initialize RFE and specify n_features_to_select=2 to select the top 2 most important features.\n",
        "* RFE recursively eliminates the least important feature at each iteration based on the model's learned weights or feature importance.\n",
        "\n",
        "**Step 5:**\n",
        "\n",
        "**Feature Selection:**\n",
        "\n",
        "After training, RFE returns the selected features (those with rfe.support_ == True).\n",
        "\n",
        "**Step 6:**\n",
        "\n",
        "**Train on Selected Features:**\n",
        "\n",
        "The model is trained using only the selected features, and the accuracy is evaluated on the test set.\n",
        "\n"
      ],
      "metadata": {
        "id": "mme4vuPAQcvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a sample dataset (Iris)\n",
        "iris = load_iris()\n",
        "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "y = iris.target\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Initialize RFE and select the top 2 features\n",
        "rfe = RFE(estimator=model, n_features_to_select=2)\n",
        "rfe.fit(X_train, y_train)\n",
        "\n",
        "# Display the results\n",
        "print(\"Selected features:\", X.columns[rfe.support_])\n",
        "\n",
        "# Use the selected features to train the model\n",
        "X_train_selected = X_train[X.columns[rfe.support_]]\n",
        "X_test_selected = X_test[X.columns[rfe.support_]]\n",
        "\n",
        "# Train the model on the selected features\n",
        "model.fit(X_train_selected, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = model.score(X_test_selected, y_test)\n",
        "print(f\"Model accuracy with selected features: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkp453BnQbac",
        "outputId": "8c59c5ea-56ff-4b46-a30d-40fb4b1027a6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected features: Index(['petal length (cm)', 'petal width (cm)'], dtype='object')\n",
            "Model accuracy with selected features: 1.0000\n"
          ]
        }
      ]
    }
  ]
}